<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="rustdoc"><meta name="description" content="Llama Core, abbreviated as `llama-core`, defines a set of APIs. Developers can utilize these APIs to build applications based on large models, such as chatbots, RAG, and more."><title>llama_core - Rust</title><script>if(window.location.protocol!=="file:")document.head.insertAdjacentHTML("beforeend","SourceSerif4-Regular-6b053e98.ttf.woff2,FiraSans-Italic-81dc35de.woff2,FiraSans-Regular-0fe48ade.woff2,FiraSans-MediumItalic-ccf7e434.woff2,FiraSans-Medium-e1aa3f0a.woff2,SourceCodePro-Regular-8badfe75.ttf.woff2,SourceCodePro-Semibold-aa29a496.ttf.woff2".split(",").map(f=>`<link rel="preload" as="font" type="font/woff2"href="../static.files/${f}">`).join(""))</script><link rel="stylesheet" href="../static.files/normalize-9960930a.css"><link rel="stylesheet" href="../static.files/rustdoc-ca0dd0c4.css"><meta name="rustdoc-vars" data-root-path="../" data-static-root-path="../static.files/" data-current-crate="llama_core" data-themes="" data-resource-suffix="" data-rustdoc-version="1.92.0-nightly (57ef8d642 2025-10-15)" data-channel="nightly" data-search-js="search-8d3311b9.js" data-stringdex-js="stringdex-828709d0.js" data-settings-js="settings-c38705f0.js" ><script src="../static.files/storage-e2aeef58.js"></script><script defer src="../crates.js"></script><script defer src="../static.files/main-ce535bd0.js"></script><noscript><link rel="stylesheet" href="../static.files/noscript-263c88ec.css"></noscript><link rel="alternate icon" type="image/png" href="../static.files/favicon-32x32-eab170b8.png"><link rel="icon" type="image/svg+xml" href="../static.files/favicon-044be391.svg"></head><body class="rustdoc mod crate"><!--[if lte IE 11]><div class="warning">This old browser is unsupported and will most likely display funky things.</div><![endif]--><rustdoc-topbar><h2><a href="#">Crate llama_core</a></h2></rustdoc-topbar><nav class="sidebar"><div class="sidebar-crate"><h2><a href="../llama_core/index.html">llama_<wbr>core</a><span class="version">0.34.0</span></h2></div><div class="sidebar-elems"><ul class="block"><li><a id="all-types" href="all.html">All Items</a></li></ul><section id="rustdoc-toc"><h3><a href="#reexports">Crate Items</a></h3><ul class="block"><li><a href="#reexports" title="Re-exports">Re-exports</a></li><li><a href="#modules" title="Modules">Modules</a></li><li><a href="#structs" title="Structs">Structs</a></li><li><a href="#enums" title="Enums">Enums</a></li><li><a href="#constants" title="Constants">Constants</a></li><li><a href="#functions" title="Functions">Functions</a></li></ul></section><div id="rustdoc-modnav"></div></div></nav><div class="sidebar-resizer" title="Drag to resize sidebar"></div><main><div class="width-limiter"><section id="main-content" class="content"><div class="main-heading"><h1>Crate <span>llama_<wbr>core</span>&nbsp;<button id="copy-path" title="Copy item path to clipboard">Copy item path</button></h1><rustdoc-toolbar></rustdoc-toolbar><span class="sub-heading"><a class="src" href="../src/llama_core/lib.rs.html#1-1028">Source</a> </span></div><details class="toggle top-doc" open><summary class="hideme"><span>Expand description</span></summary><div class="docblock"><p>Llama Core, abbreviated as <code>llama-core</code>, defines a set of APIs. Developers can utilize these APIs to build applications based on large models, such as chatbots, RAG, and more.</p>
</div></details><h2 id="reexports" class="section-header">Re-exports<a href="#reexports" class="anchor">§</a></h2><dl class="item-table reexports"><dt id="reexport.LlamaCoreError"><code>pub use error::<a class="enum" href="error/enum.LlamaCoreError.html" title="enum llama_core::error::LlamaCoreError">LlamaCoreError</a>;</code></dt><dt id="reexport.EngineType"><code>pub use graph::<a class="enum" href="graph/enum.EngineType.html" title="enum llama_core::graph::EngineType">EngineType</a>;</code></dt><dt id="reexport.Graph"><code>pub use graph::<a class="struct" href="graph/struct.Graph.html" title="struct llama_core::graph::Graph">Graph</a>;</code></dt><dt id="reexport.GraphBuilder"><code>pub use graph::<a class="struct" href="graph/struct.GraphBuilder.html" title="struct llama_core::graph::GraphBuilder">GraphBuilder</a>;</code></dt><dt id="reexport.GgmlMetadata"><code>pub use metadata::ggml::<a class="struct" href="metadata/ggml/struct.GgmlMetadata.html" title="struct llama_core::metadata::ggml::GgmlMetadata">GgmlMetadata</a>;</code></dt><dt id="reexport.GgmlTtsMetadata"><code>pub use metadata::ggml::<a class="struct" href="metadata/ggml/struct.GgmlTtsMetadata.html" title="struct llama_core::metadata::ggml::GgmlTtsMetadata">GgmlTtsMetadata</a>;</code></dt><dt id="reexport.PiperMetadata"><code>pub use metadata::piper::<a class="struct" href="metadata/piper/struct.PiperMetadata.html" title="struct llama_core::metadata::piper::PiperMetadata">PiperMetadata</a>;</code></dt><dt id="reexport.BaseMetadata"><code>pub use metadata::<a class="trait" href="metadata/trait.BaseMetadata.html" title="trait llama_core::metadata::BaseMetadata">BaseMetadata</a>;</code></dt></dl><h2 id="modules" class="section-header">Modules<a href="#modules" class="anchor">§</a></h2><dl class="item-table"><dt><a class="mod" href="audio/index.html" title="mod llama_core::audio">audio</a></dt><dt><a class="mod" href="chat/index.html" title="mod llama_core::chat">chat</a></dt><dt><a class="mod" href="completions/index.html" title="mod llama_core::completions">completions</a></dt><dd>Define APIs for completions.</dd><dt><a class="mod" href="embeddings/index.html" title="mod llama_core::embeddings">embeddings</a></dt><dd>Define APIs for computing embeddings.</dd><dt><a class="mod" href="error/index.html" title="mod llama_core::error">error</a></dt><dd>Error types for the Llama Core library.</dd><dt><a class="mod" href="files/index.html" title="mod llama_core::files">files</a></dt><dd>Define APIs for file operations.</dd><dt><a class="mod" href="graph/index.html" title="mod llama_core::graph">graph</a></dt><dd>Define Graph and GraphBuilder APIs for creating a new computation graph.</dd><dt><a class="mod" href="images/index.html" title="mod llama_core::images">images</a></dt><dd>Define APIs for image generation and edit.</dd><dt><a class="mod" href="metadata/index.html" title="mod llama_core::metadata">metadata</a></dt><dd>Define the types for model metadata.</dd><dt><a class="mod" href="models/index.html" title="mod llama_core::models">models</a></dt><dd>Define APIs for querying models.</dd><dt><a class="mod" href="tts/index.html" title="mod llama_core::tts">tts</a></dt><dt><a class="mod" href="utils/index.html" title="mod llama_core::utils">utils</a></dt><dd>Define utility functions.</dd></dl><h2 id="structs" class="section-header">Structs<a href="#structs" class="anchor">§</a></h2><dl class="item-table"><dt><a class="struct" href="struct.PluginInfo.html" title="struct llama_core::PluginInfo">Plugin<wbr>Info</a></dt><dd>Version info of the <code>wasi-nn_ggml</code> plugin, including the build number and the commit id.</dd></dl><h2 id="enums" class="section-header">Enums<a href="#enums" class="anchor">§</a></h2><dl class="item-table"><dt><a class="enum" href="enum.StableDiffusionTask.html" title="enum llama_core::StableDiffusionTask">Stable<wbr>Diffusion<wbr>Task</a></dt><dd>The task type of the stable diffusion context</dd></dl><h2 id="constants" class="section-header">Constants<a href="#constants" class="anchor">§</a></h2><dl class="item-table"><dt><a class="constant" href="constant.ARCHIVES_DIR.html" title="constant llama_core::ARCHIVES_DIR">ARCHIVES_<wbr>DIR</a></dt><dd>The directory for storing the archives in wasm virtual file system.</dd></dl><h2 id="functions" class="section-header">Functions<a href="#functions" class="anchor">§</a></h2><dl class="item-table"><dt><a class="fn" href="fn.get_plugin_info.html" title="fn llama_core::get_plugin_info">get_<wbr>plugin_<wbr>info</a></dt><dd>Get the plugin info</dd><dt><a class="fn" href="fn.init_ggml_chat_context.html" title="fn llama_core::init_ggml_chat_context">init_<wbr>ggml_<wbr>chat_<wbr>context</a></dt><dd>Initialize the ggml context</dd><dt><a class="fn" href="fn.init_ggml_embeddings_context.html" title="fn llama_core::init_ggml_embeddings_context">init_<wbr>ggml_<wbr>embeddings_<wbr>context</a></dt><dd>Initialize the ggml context</dd><dt><a class="fn" href="fn.init_ggml_tts_context.html" title="fn llama_core::init_ggml_tts_context">init_<wbr>ggml_<wbr>tts_<wbr>context</a></dt><dd>Initialize the ggml context for TTS scenarios.</dd><dt><a class="fn" href="fn.init_piper_context.html" title="fn llama_core::init_piper_context">init_<wbr>piper_<wbr>context</a></dt><dd>Initialize the piper context</dd><dt><a class="fn" href="fn.init_sd_context_with_full_model.html" title="fn llama_core::init_sd_context_with_full_model">init_<wbr>sd_<wbr>context_<wbr>with_<wbr>full_<wbr>model</a></dt><dd>Initialize the stable-diffusion context with the given full diffusion model</dd><dt><a class="fn" href="fn.init_sd_context_with_standalone_model.html" title="fn llama_core::init_sd_context_with_standalone_model">init_<wbr>sd_<wbr>context_<wbr>with_<wbr>standalone_<wbr>model</a></dt><dd>Initialize the stable-diffusion context with the given standalone diffusion model</dd><dt><a class="fn" href="fn.init_whisper_context.html" title="fn llama_core::init_whisper_context">init_<wbr>whisper_<wbr>context</a><wbr><span class="stab portability" title="Available on crate feature `whisper` only"><code>whisper</code></span></dt><dd>Initialize the whisper context</dd><dt><a class="fn" href="fn.running_mode.html" title="fn llama_core::running_mode">running_<wbr>mode</a></dt><dd>Return the current running mode.</dd></dl></section></div></main></body></html>