[server]
socket_addr = "0.0.0.0:8080" # Socket address to listen on.
# Default is "0.0.0.0:8080".


[chat]
model_name      = "Llama-3.2-1B" # Name of the chat model. Default is "default".
model_alias     = "default"      # Alias of the chat model. Default is "default".
ctx_size        = 4096           # Context size. Default is 4096.
batch_size      = 512            # Batch size. Default is 512.
ubatch_size     = 512            # Physical maximum batch size. Default is 512.
prompt_template = "llama-3-chat" # Required for running chat model. "none" means
# no prompt template, which should be replaced
# by the prompt template according to the model
# to use.
reverse_prompt = "" # Halt generation at PROMPT, return control. Default is
# empty string.
n_predict = -1 # Number of tokens to predict. -1 = infinity,
# -2 = until context filled. Default is -1.
n_gpu_layers = 100     # Number of layers to run on GPU. Default is 100.
split_mode   = "layer" # Split the model across multiple GPUs.
# Possible values:
# - `none` (use one GPU only),
# - `layer` (split layers and KV across GPUs, default),
# - `row` (split rows across GPUs) [default: layer]
main_gpu     = 0     # Main GPU to use. Default is 0.
tensor_split = "3:2" # How split tensors should be distributed accross GPUs.
# If null the model is not split; otherwise, a
# comma-separated list of non-negative values, e.g.,
# "3,2" presents 60% of the data to GPU 0 and 40% to
# GPU 1.
threads = 2 # Number of threads to use during computation.
#Default is 2.
no_mmap = true # Disable memory mapping for file access of chat model.
# Default is true.
temp              = 1.0 # Temperature. Default is 1.0.
top_p             = 1.0 # Top-p. Default is 1.0.
repeat_penalty    = 1.1 # Repeat penalty. Default is 1.1.
presence_penalty  = 0.0 # Presence penalty. Default is 0.0.
frequency_penalty = 0.0 # Frequency penalty. Default is 0.0.
grammar           = ""  # BNF-like grammar to constrain generations.
# Default is empty string.
json_schema = "" # JSON schema to constrain generations
# (<https://json-schema.org/>), e.g. `{}` for any JSON
# object. For schemas w/ external $refs, use --grammar
# + example/json_schema_to_grammar.py instead.
llava_mmproj = "" # Path to the multimodal projector file.
# Default is empty string.
include_usage = false # Whether to include token usage in the stream
# response. Defaults to false.

[embedding]
model_name  = "nomic-embed-text-v1.5" # Name of the embedding model. Default is "default".
model_alias = "embedding"             # Alias of the embedding model. Default is "embedding".
ctx_size    = 384                     # Context size. Default is 384.
batch_size  = 512                     # Batch size. Default is 512.
ubatch_size = 512                     # Physical maximum batch size. Default is 512.
split_mode  = "layer"                 # Split the model across multiple GPUs.
# Possible values:
# - `none` (use one GPU only),
# - `layer` (split layers and KV across GPUs, default),
# - `row` (split rows across GPUs) [default: layer]
main_gpu     = 0     # Main GPU to use. Default is 0.
tensor_split = "3:2" # How split tensors should be distributed accross GPUs.
# If null the model is not split; otherwise, a
# comma-separated list of non-negative values, e.g.,
# "3,2" presents 60% of the data to GPU 0 and 40% to GPU 1.
threads = 2 # Number of threads to use during computation.
# Default is 2.

[tts]
model_name  = "OuteTTS-0.2-500M"                    # Name of the TTS model. Default is "default".
model_alias = "tts"                                 # Alias of the TTS model. Default is "tts".
codec_model = "wavtokenizer-large-75-ggml-f16.gguf" # Required. Path to the codec
# model file.
speaker_file = ""   # Path to an alternative speaker file.
ctx_size     = 4096 # Context size. Default is 8192.
batch_size   = 4096 # Batch size. Default is 8192.
ubatch_size  = 4096 # Physical maximum batch size. Default is 8192.
n_predict    = 4096 # Number of tokens to predict. Default is 4096.
n_gpu_layers = 100  # Number of layers to run on GPU. Default is 100.
temp         = 0.8  # Temperature. Default is 0.8.
