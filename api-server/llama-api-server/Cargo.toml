[package]
name = "llama-api-server"
version = "0.3.3"
edition = "2021"

[dependencies]
futures = { version = "0.3.6", default-features = false, features = ["async-await", "std"] }
serde.workspace = true
serde_json = "1.0"
endpoints.workspace = true
chat-prompts.workspace = true
serde_yaml = "0.9"
hyper_wasi = { version = "0.15", features = ["full"] }
tokio_wasi = { version = "1", features = ["full"] }
wasi-nn = { git = "https://github.com/second-state/wasmedge-wasi-nn", branch = "refactor-execution-context" }
thiserror.workspace = true
uuid = { version = "1.4", features = ["v4", "fast-rng", "macro-diagnostics"] }
clap = { version = "4.4.6", features = ["cargo"] }
once_cell = "1.18"
mime_guess = "2.0.4"
futures-util = "0.3"
llama-core = { path = "../llama-core", version = "^0.2" }
