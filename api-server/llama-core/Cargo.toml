[package]
name = "llama-core"
version = "0.11.3"
edition = "2021"
readme = "README.md"
repository = "https://github.com/LlamaEdge/LlamaEdge"
license = "Apache-2.0"
documentation = "https://docs.rs/llama-core/"
categories = ["wasm", "science"]
description = "The core component of LlamaEdge"

[dependencies]
endpoints.workspace = true
chat-prompts.workspace = true
wasmedge-wasi-nn = { git = "https://github.com/second-state/wasmedge-wasi-nn.git", branch = "burn" }
thiserror.workspace = true
serde.workspace = true
serde_json.workspace = true
uuid.workspace = true
once_cell = "1.18"
futures = { version = "0.3.6", default-features = false, features = ["async-await", "std"] }
futures-util = "0.3"
reqwest = { package = "reqwest_wasi", version = "0.11", features = ["json", "stream"] }
qdrant_rest_client = { version = "0.0.4", default-features = false }
text-splitter = { version = "^0.7", features = ["tiktoken-rs", "markdown"] }
tiktoken-rs = "^0.5"
wasi-logger = { workspace = true, optional = true }
log = { workspace = true, optional = true }

[features]
default = []
full = ["https", "logging"]
https = ["reqwest/wasmedge-tls", "qdrant_rest_client/wasmedge-tls"]
logging = ["wasi-logger", "log"]
